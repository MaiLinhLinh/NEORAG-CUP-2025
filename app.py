import streamlit as st
from embeddings import Embeddings
from vector_db import VectorDatabase
import pandas as pd
import os
from docx import Document
import numpy as np
import time
from functools import lru_cache
import openai
from dotenv import load_dotenv
from semantic_router.route import Route
from semantic_router.router import SemanticRouter
from semantic_router.samples import info_CLBSample
from semantic_router.samples import chitchatSample
from collections import defaultdict
from reflection import Reflection

load_dotenv()

@st.cache_resource
def _get_reranker():
    from rerank import Reranker           # import trong hÃ m Ä‘á»ƒ trÃ¡nh import vÃ²ng & náº·ng
    return Reranker()                     # chá»‰ táº¡o 1 láº§n duy nháº¥t

# Khá»Ÿi táº¡o session_state Ä‘á»ƒ lÆ°u há»™i thoáº¡i

def init_session():

    
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {
                "role":"system",
                "content": """Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn cung cáº¥p thÃ´ng tin vá» CÃ¢u láº¡c bá»™ Láº­p trÃ¬nh ProPTIT.
                Báº¡n sáº½ nháº­n Ä‘Æ°á»£c dá»¯ liá»‡u ngá»¯ cáº£nh (context) tá»« má»™t há»‡ thá»‘ng Retrieval-Augmented Generation (RAG) chá»©a cÃ¡c thÃ´ng tin chÃ­nh xÃ¡c vá» CLB.

                NguyÃªn táº¯c tráº£ lá»i:
                1. Chá»‰ sá»­ dá»¥ng thÃ´ng tin tá»« context Ä‘Æ°á»£c cung cáº¥p Ä‘á»ƒ tráº£ lá»i. Context sáº½ Ä‘Æ°á»£c cung cáº¥p á»Ÿ Ä‘áº§u má»—i query cá»§a ngÆ°á»i dÃ¹ng. CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng náº±m á»Ÿ Ä‘áº§u. 
                2. Náº¿u ngÆ°á»i dÃ¹ng há»i cÃ¢u há»i khÃ´ng liÃªn quan Ä‘áº¿n CLB ProPTIT, hÃ£y tráº£ lá»i nhÆ° bÃ¬nh thÆ°á»ng, nhÆ°ng khÃ´ng sá»­ dá»¥ng thÃ´ng tin tá»« context.
                3. TrÃ¬nh bÃ y cÃ¢u tráº£ lá»i rÃµ rÃ ng, dá»… hiá»ƒu. CÃ³ thá»ƒ sá»­ dá»¥ng emoij icon khi cáº§n.
                4. Tuyá»‡t Ä‘á»‘i khÃ´ng suy Ä‘oÃ¡n hoáº·c bá»‹a thÃ´ng tin.
                5. Giá»¯ phong cÃ¡ch tráº£ lá»i thÃ¢n thiá»‡n, chuyÃªn nghiá»‡p vÃ  nháº¥t quÃ¡n.
                6. Trong context cÃ³ thá»ƒ chá»©a nhiá»u thÃ´ng tin khÃ¡c nhau, hÃ£y táº­p trung vÃ o cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng Ä‘á»ƒ tráº£ lá»i chÃ­nh xÃ¡c nháº¥t.

                Nhiá»‡m vá»¥ cá»§a báº¡n:
                - Tráº£ lá»i cÃ¡c cÃ¢u há»i vá» CLB Láº­p trÃ¬nh ProPTIT: lá»‹ch sá»­, thÃ nh viÃªn, hoáº¡t Ä‘á»™ng, sá»± kiá»‡n, dá»± Ã¡n, ná»™i quy, thÃ nh viÃªn tiÃªu biá»ƒu, vÃ  cÃ¡c thÃ´ng tin liÃªn quan khÃ¡c.
                - Náº¿u cÃ¢u há»i khÃ´ng liÃªn quan Ä‘áº¿n CLB ProPTIT thÃ¬ trong cÃ¢u tráº£ lá»i khÃ´ng bao gá»“m báº¥t cá»© tá»« ngá»¯, thÃ´ng tin vá» CLB.
                """
            }
        ]
    if "history" not in st.session_state:
        st.session_state.history = []
    

# HÃ m load dá»¯ liá»‡u vÃ  setup
@st.cache_resource
def setup():

    openai.api_key = os.getenv("OPENAI_API_KEY")

    reranker = _get_reranker()

    
    vector_db = VectorDatabase(db_type= "mongodb")

    embedding = Embeddings(model_name= "text-embedding-3-large", type= "openai") 

    routes = [
        Route(name="info_CLB", samples=info_CLBSample),
        Route(name="chitchat", samples=chitchatSample)
    ]
    router = SemanticRouter(embedding, routes)
    
    return embedding, vector_db, router,reranker
    

#HÃ m xá»­ lÃ­ truy váº¥n ngÆ°á»i dÃ¹ng
def handle_query(query, embedding, vector_db, reranker, router):

    openai.api_key = os.getenv("OPENAI_API_KEY")


    #Sá»­ dá»¥ng semantic_router

    
    route_result = router.guide(query)
    best_route = route_result[1] # láº¥y tÃªn router
    if best_route == "uncertain":
        best_route = "chitchat"
        messages_for_answer = [
            {"role": "system", "content": st.session_state.messages[0]["content"]},
            {"role": "user", "content": query}
        ]
    st.chat_message("assistant").markdown(f"**[Äá»‹nh tuyáº¿n]:** `{best_route}`")
    if best_route == "info_CLB":
        reflection = Reflection(openai)

        rewritten_query = reflection._rewrite(st.session_state.messages, query)
    
        #embedding cÃ¢u há»i
        # Táº¡o embedding cho cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng
        user_embedding = embedding.encode(rewritten_query)

            # TÃ¬m kiáº¿m thÃ´ng tin liÃªn quan trong cÆ¡ sá»Ÿ dá»¯ liá»‡u
        results = vector_db.query("information",user_embedding,limit= 60)
           
            # Rerank
        print("Káº¿t quáº£ trÆ°á»›c rerank:")

        cnt = 0
        for result in results:
            print (f"VÄƒn báº£n sá»‘: {cnt+1}")
            print(f"Title: {result['title']}")
            print (f"Information: {result['information']}")
            print("-" *50)
            cnt+=1
            if cnt == 5:
                break
        
        passages = [result["information"] for result in results]
        scores, reranker_passages = reranker(rewritten_query, passages)

            # (5) Map ngÆ°á»£c passage -> result gá»‘c Ä‘á»ƒ khÃ´ng máº¥t title/metadata
            #    DÃ¹ng dict {passage: [list cÃ¡c index]} Ä‘á»ƒ xá»­ lÃ½ trÆ°á»ng há»£p passage trÃ¹ng nhau
        passage2idxs = defaultdict(list)
        for idx, p in enumerate(passages):
            passage2idxs[p].append(idx)

        reranked_results = []
        for s, p in zip(scores,  reranker_passages):
            if passage2idxs[p]:
                idx = passage2idxs[p].pop(0)   # láº¥y index Ä‘áº§u tiÃªn khá»›p passage nÃ y
                r = results[idx].copy()        # copy Ä‘á»ƒ khÃ´ng sá»­a object gá»‘c (tuá»³ báº¡n)
                r["_rerank_score"] = float(s)
                reranked_results.append(r)
            
        reranked_results = reranked_results[:15]
        
            # In káº¿t quáº£ sau reranking
        dem = 0
        print("\nğŸ“Š Káº¿t quáº£ sau khi rerank:")
        for i, r in enumerate (reranked_results):
            print(f"VÄƒn báº£n{i+1} | Score: {r.get('_rerank_score', 0):.4f}")
            print(r.get('information', ''))
            print("-" *50)
            dem+= 1
            if dem == 5:
                break
           

            #GhÃ©p cÃ¡c Ä‘oáº¡n tÃ¬m Ä‘Æ°á»£c thÃ nh má»™t khá»‘i 'context' vÄƒn báº£n pháº³ng
        context = "\n".join(result["information"] for result in reranked_results)
        
        base_prompt = st.session_state.messages[0]["content"]
        
        new_context = base_prompt + f"\nCÃ¢u há»i: {rewritten_query}\n" + f"\nThÃ´ng tin liÃªn quan:\n{context}"
        #st.session_state.messages[0]["content"] = new_context    
        
        messages_for_answer = [
            {"role": "system", "content": base_prompt},
            {"role": "user",
            "content": new_context}
        ]
         
    #else:
    #    st.session_state.messages.append({"role":"user", "content": query})
    
    # Gá»i openAI dáº¡ng stream
    response_stream = embedding.client.chat.completions.create(
        model = "gpt-4o-mini",
        messages=messages_for_answer,
        stream=True
    )
    with st.chat_message("assistant"):
        assitant_reply = ""
        placeholder = st.empty()

        for chunk in response_stream:
            if chunk.choices and chunk.choices[0].delta.content:
                token = chunk.choices[0].delta.content
                assitant_reply += token
                placeholder.markdown(assitant_reply +"â–Œ")
        placeholder.markdown(assitant_reply)
    st.session_state.messages.append({"role": "assistant", "content": assitant_reply})
    
    return None

       
    
#--- Giao diá»‡n Streamlit----

st.title("Chat Bot")

init_session()
embedding, vector_db,router,reranker = setup()


# Hiá»ƒn thá»‹ há»™i thoáº¡i cÅ©
for message in st.session_state.messages[1:]:
    with st.chat_message(message["role"]):
       st.markdown(message["content"])

user_input = st.chat_input("Anh/chá»‹ muá»‘n há»i gÃ¬?")
if user_input:
    with st.chat_message("user"):
        st.markdown(user_input)
    st.session_state.messages.append({"role": "user", "content": user_input})

    handle_query(user_input, embedding, vector_db,reranker, router)
    